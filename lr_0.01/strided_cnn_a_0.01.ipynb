{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"strided_cnn_a_0.01.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ae3e741221bc406fa6f7cca74a3d4633":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e42b92bdfa744515b94d1cc2fa08c427","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9333cf4f422542249d60a4ee965bc7f5","IPY_MODEL_ecb509774e0a4f36bc21f9e7a60cce3d"]}},"e42b92bdfa744515b94d1cc2fa08c427":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9333cf4f422542249d60a4ee965bc7f5":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1b8624078c04404aa5ea8893da8de2cc","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_28e65bcfafe7447fa4d4b2502f47a9f4"}},"ecb509774e0a4f36bc21f9e7a60cce3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aab45a74f9dc4c70af3e9c9d5aaafa22","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 63626011.96it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_68601a240a9f46b19fc36bd3decc568e"}},"1b8624078c04404aa5ea8893da8de2cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"28e65bcfafe7447fa4d4b2502f47a9f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aab45a74f9dc4c70af3e9c9d5aaafa22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"68601a240a9f46b19fc36bd3decc568e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["[View in Colaboratory](https://colab.research.google.com/github/StefOe/all-conv-pytorch/blob/master/cifar10.ipynb)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NjsA9TTlvXU7","colab":{}},"source":["cuda = True\n","train_batch_size = 32\n","test_batch_size = 124\n","best_loss = float(\"inf\")\n","best_epoch = -1\n","dataset_path = './cifar10'\n","gsync_save = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"z0LLDiZnvXU-","colab":{}},"source":["try:\n","    import torch\n","except ModuleNotFoundError:\n","    from os import path\n","    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","    accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.1-{platform}-linux_x86_64.whl\n","    import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g_41e1ZYvXVB","colab":{}},"source":["try:\n","    import torchvision\n","except ModuleNotFoundError:\n","    !pip install -q torchvision\n","\n","from torchvision import datasets, transforms"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Z87VBSpCp3ld","outputId":"ae48c7d5-bb83-4535-8539-7f166960dbca","executionInfo":{"status":"ok","timestamp":1584963712210,"user_tz":-60,"elapsed":13084,"user":{"displayName":"Varun S","photoUrl":"https://lh5.googleusercontent.com/-WGxXrzLzs2c/AAAAAAAAAAI/AAAAAAAAEug/Tfais7qtF5I/s64/photo.jpg","userId":"10851301472990348824"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["from torchsummary import summary\n","\n","class AllConvNet(nn.Module):\n","    def __init__(self, input_size, n_classes=10, **kwargs):\n","        super(AllConvNet, self).__init__()\n","        self.conv1 = nn.Conv2d(input_size, 96, 5, padding=1,stride=2)\n","        self.conv3 = nn.Conv2d(96, 192, 5, padding=1,stride=2)\n","        self.conv5 = nn.Conv2d(192, 192, 3, padding=1)\n","        self.conv6 = nn.Conv2d(192, 192, 1)\n","        self.class_conv = nn.Conv2d(192, n_classes, 1)\n","\n","\n","    def forward(self, x):\n","        x_drop = F.dropout(x, .2)\n","        conv1_out = F.relu(self.conv1(x_drop))\n","        conv3_out = F.relu(self.conv3(conv1_out))\n","        conv5_out = F.relu(self.conv5(conv3_out))\n","        conv6_out = F.relu(self.conv6(conv5_out))\n","        class_out = F.relu(self.class_conv(conv6_out))\n","        pool_out = F.adaptive_avg_pool2d(class_out, 1)\n","        pool_out.squeeze_(-1)\n","        pool_out.squeeze_(-1)\n","        return pool_out\n","\n","trial=AllConvNet(3)\n","print(trial)\n","\n","summary(trial.cuda(),(3,32,32))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["AllConvNet(\n","  (conv1): Conv2d(3, 96, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n","  (conv3): Conv2d(96, 192, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n","  (conv5): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv6): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n","  (class_conv): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))\n",")\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 96, 15, 15]           7,296\n","            Conv2d-2            [-1, 192, 7, 7]         460,992\n","            Conv2d-3            [-1, 192, 7, 7]         331,968\n","            Conv2d-4            [-1, 192, 7, 7]          37,056\n","            Conv2d-5             [-1, 10, 7, 7]           1,930\n","================================================================\n","Total params: 839,242\n","Trainable params: 839,242\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 0.38\n","Params size (MB): 3.20\n","Estimated Total Size (MB): 3.60\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rZu8tSSYvXVL","outputId":"475a5733-cfb3-4c1f-9869-231523082902","executionInfo":{"status":"ok","timestamp":1584963720491,"user_tz":-60,"elapsed":21354,"user":{"displayName":"Varun S","photoUrl":"https://lh5.googleusercontent.com/-WGxXrzLzs2c/AAAAAAAAAAI/AAAAAAAAEug/Tfais7qtF5I/s64/photo.jpg","userId":"10851301472990348824"}},"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["ae3e741221bc406fa6f7cca74a3d4633","e42b92bdfa744515b94d1cc2fa08c427","9333cf4f422542249d60a4ee965bc7f5","ecb509774e0a4f36bc21f9e7a60cce3d","1b8624078c04404aa5ea8893da8de2cc","28e65bcfafe7447fa4d4b2502f47a9f4","aab45a74f9dc4c70af3e9c9d5aaafa22","68601a240a9f46b19fc36bd3decc568e"]}},"source":["cuda = cuda and torch.cuda.is_available()\n","trainset = datasets.CIFAR10(root=dataset_path, train=True, download=True)\n","train_mean = trainset.data.mean(axis=(0,1,2))/255  # [0.49139968  0.48215841  0.44653091]\n","train_std = trainset.data.std(axis=(0,1,2))/255  # [0.24703223  0.24348513  0.26158784]\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","   transforms.Normalize(train_mean, train_std),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(train_mean, train_std),\n","])\n","kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n","train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(\n","    root=dataset_path, train=True, download=True,\n","    transform=transform_train),\n","    batch_size=train_batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10(root=dataset_path, train=False, download=True,\n","    transform=transform_test),\n","    batch_size=test_batch_size, shuffle=False, **kwargs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae3e741221bc406fa6f7cca74a3d4633","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./cifar10/cifar-10-python.tar.gz to ./cifar10\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3WISJzjRvXVP","colab":{}},"source":["model = AllConvNet(3)\n","if cuda:\n","    model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay = 0.001)\n","scheduler = optim.lr_scheduler.MultiStepLR(\n","    optimizer, milestones=[200, 250, 300], gamma=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5shX88fvvXVR","colab":{}},"source":["def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        if cuda:\n","            data, target = data.cuda(), target.cuda()\n","        data, target = Variable(data), Variable(target)\n","\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.data.item()))\n","            \n","def test(epoch, best_loss, best_epoch):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        if cuda:\n","            data, target = data.cuda(), target.cuda()\n","        data, target = Variable(data), Variable(target)\n","\n","        output = model(data)\n","        # sum up batch loss\n","        test_loss += criterion(output, target).data.item()\n","        # get the index of the max log-probability\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print(\n","        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","            test_loss, correct, len(test_loader.dataset), 100. * correct /\n","            len(test_loader.dataset)))\n","    \n","    if test_loss < best_loss:\n","        best_epoch = epoch\n","        best_loss = test_loss\n","        torch.save(model, \"best.pt\")\n","        #if gsync_save:\n","         #   gsync.update_file_to_folder(\"best.pt\")\n","    return best_loss, best_epoch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q_YHklUQvXVU","outputId":"037ce411-e390-4267-d018-da46fe9a88e0","executionInfo":{"status":"error","timestamp":1584947936567,"user_tz":-60,"elapsed":1123,"user":{"displayName":"Varun S","photoUrl":"https://lh5.googleusercontent.com/-WGxXrzLzs2c/AAAAAAAAAAI/AAAAAAAAEug/Tfais7qtF5I/s64/photo.jpg","userId":"10851301472990348824"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for epoch in range(350):\n","    scheduler.step()\n","    train(epoch)\n","    best_loss, best_epoch = test(epoch, best_loss, best_epoch)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.298792\n","Train Epoch: 0 [3200/50000 (6%)]\tLoss: 2.207819\n","Train Epoch: 0 [6400/50000 (13%)]\tLoss: 2.189686\n","Train Epoch: 0 [9600/50000 (19%)]\tLoss: 2.199766\n","Train Epoch: 0 [12800/50000 (26%)]\tLoss: 2.176384\n","Train Epoch: 0 [16000/50000 (32%)]\tLoss: 1.877271\n","Train Epoch: 0 [19200/50000 (38%)]\tLoss: 2.253075\n","Train Epoch: 0 [22400/50000 (45%)]\tLoss: 2.023848\n","Train Epoch: 0 [25600/50000 (51%)]\tLoss: 2.228978\n","Train Epoch: 0 [28800/50000 (58%)]\tLoss: 2.066344\n","Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.822839\n","Train Epoch: 0 [35200/50000 (70%)]\tLoss: 1.982479\n","Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.969162\n","Train Epoch: 0 [41600/50000 (83%)]\tLoss: 2.161825\n","Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.851367\n","Train Epoch: 0 [48000/50000 (96%)]\tLoss: 2.206217\n","\n","Test set: Average loss: 0.0159, Accuracy: 2975/10000 (30%)\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type AllConvNet. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.721530\n","Train Epoch: 1 [3200/50000 (6%)]\tLoss: 1.952425\n","Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.734321\n","Train Epoch: 1 [9600/50000 (19%)]\tLoss: 1.722837\n","Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.942697\n","Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.732368\n","Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.992833\n","Train Epoch: 1 [22400/50000 (45%)]\tLoss: 1.744365\n","Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.735537\n","Train Epoch: 1 [28800/50000 (58%)]\tLoss: 1.850303\n","Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.532899\n","Train Epoch: 1 [35200/50000 (70%)]\tLoss: 1.436706\n","Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.469779\n","Train Epoch: 1 [41600/50000 (83%)]\tLoss: 1.679826\n","Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.494148\n","Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.616164\n","\n","Test set: Average loss: 0.0126, Accuracy: 4360/10000 (44%)\n","\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.819349\n","Train Epoch: 2 [3200/50000 (6%)]\tLoss: 1.603708\n","Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.655199\n","Train Epoch: 2 [9600/50000 (19%)]\tLoss: 1.566849\n","Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.530027\n","Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.340109\n","Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.472734\n","Train Epoch: 2 [22400/50000 (45%)]\tLoss: 1.373364\n","Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.350190\n","Train Epoch: 2 [28800/50000 (58%)]\tLoss: 1.162047\n","Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.464286\n","Train Epoch: 2 [35200/50000 (70%)]\tLoss: 1.892824\n","Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.437026\n","Train Epoch: 2 [41600/50000 (83%)]\tLoss: 1.482236\n","Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.304227\n","Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.486935\n","\n","Test set: Average loss: 0.0114, Accuracy: 4867/10000 (49%)\n","\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.373437\n","Train Epoch: 3 [3200/50000 (6%)]\tLoss: 1.353314\n","Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.227618\n","Train Epoch: 3 [9600/50000 (19%)]\tLoss: 1.668489\n","Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.397691\n","Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1.533854\n","Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.443111\n","Train Epoch: 3 [22400/50000 (45%)]\tLoss: 1.422444\n","Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.303586\n","Train Epoch: 3 [28800/50000 (58%)]\tLoss: 1.621089\n","Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.444108\n","Train Epoch: 3 [35200/50000 (70%)]\tLoss: 1.175555\n","Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.212002\n","Train Epoch: 3 [41600/50000 (83%)]\tLoss: 1.226460\n","Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.258385\n","Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.454423\n","\n","Test set: Average loss: 0.0112, Accuracy: 5072/10000 (51%)\n","\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.618634\n","Train Epoch: 4 [3200/50000 (6%)]\tLoss: 1.378617\n","Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.378516\n","Train Epoch: 4 [9600/50000 (19%)]\tLoss: 1.525692\n","Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.294810\n","Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1.620492\n","Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.991853\n","Train Epoch: 4 [22400/50000 (45%)]\tLoss: 1.175593\n","Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.401908\n","Train Epoch: 4 [28800/50000 (58%)]\tLoss: 1.331309\n","Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.444253\n","Train Epoch: 4 [35200/50000 (70%)]\tLoss: 1.351040\n","Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.315311\n","Train Epoch: 4 [41600/50000 (83%)]\tLoss: 1.089469\n","Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.190809\n","Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1.170199\n","\n","Test set: Average loss: 0.0098, Accuracy: 5698/10000 (57%)\n","\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.300192\n","Train Epoch: 5 [3200/50000 (6%)]\tLoss: 1.575523\n","Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.593836\n","Train Epoch: 5 [9600/50000 (19%)]\tLoss: 1.465859\n","Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.212023\n","Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1.432724\n","Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.012061\n","Train Epoch: 5 [22400/50000 (45%)]\tLoss: 0.951749\n","Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.921326\n","Train Epoch: 5 [28800/50000 (58%)]\tLoss: 1.335906\n","Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.051321\n","Train Epoch: 5 [35200/50000 (70%)]\tLoss: 1.160904\n","Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.460272\n","Train Epoch: 5 [41600/50000 (83%)]\tLoss: 1.461384\n","Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.101597\n","Train Epoch: 5 [48000/50000 (96%)]\tLoss: 1.184175\n","\n","Test set: Average loss: 0.0100, Accuracy: 5587/10000 (56%)\n","\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.201403\n","Train Epoch: 6 [3200/50000 (6%)]\tLoss: 1.385053\n","Train Epoch: 6 [6400/50000 (13%)]\tLoss: 1.192481\n","Train Epoch: 6 [9600/50000 (19%)]\tLoss: 1.216624\n","Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.653849\n","Train Epoch: 6 [16000/50000 (32%)]\tLoss: 1.262447\n","Train Epoch: 6 [19200/50000 (38%)]\tLoss: 1.051905\n","Train Epoch: 6 [22400/50000 (45%)]\tLoss: 0.757039\n","Train Epoch: 6 [25600/50000 (51%)]\tLoss: 1.214291\n","Train Epoch: 6 [28800/50000 (58%)]\tLoss: 1.455999\n","Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.996864\n","Train Epoch: 6 [35200/50000 (70%)]\tLoss: 1.294946\n","Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.898781\n","Train Epoch: 6 [41600/50000 (83%)]\tLoss: 1.193258\n","Train Epoch: 6 [44800/50000 (90%)]\tLoss: 1.283010\n","Train Epoch: 6 [48000/50000 (96%)]\tLoss: 1.201319\n","\n","Test set: Average loss: 0.0090, Accuracy: 6120/10000 (61%)\n","\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.854701\n","Train Epoch: 7 [3200/50000 (6%)]\tLoss: 0.937389\n","Train Epoch: 7 [6400/50000 (13%)]\tLoss: 1.104633\n","Train Epoch: 7 [9600/50000 (19%)]\tLoss: 1.081619\n","Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.960797\n","Train Epoch: 7 [16000/50000 (32%)]\tLoss: 1.027698\n","Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.966691\n","Train Epoch: 7 [22400/50000 (45%)]\tLoss: 1.176629\n","Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.118526\n","Train Epoch: 7 [28800/50000 (58%)]\tLoss: 1.459824\n","Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.173701\n","Train Epoch: 7 [35200/50000 (70%)]\tLoss: 1.075624\n","Train Epoch: 7 [38400/50000 (77%)]\tLoss: 1.059828\n","Train Epoch: 7 [41600/50000 (83%)]\tLoss: 1.269987\n","Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.900712\n","Train Epoch: 7 [48000/50000 (96%)]\tLoss: 1.399770\n","\n","Test set: Average loss: 0.0084, Accuracy: 6306/10000 (63%)\n","\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.879315\n","Train Epoch: 8 [3200/50000 (6%)]\tLoss: 1.106839\n","Train Epoch: 8 [6400/50000 (13%)]\tLoss: 1.450945\n","Train Epoch: 8 [9600/50000 (19%)]\tLoss: 1.117108\n","Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.987148\n","Train Epoch: 8 [16000/50000 (32%)]\tLoss: 0.824842\n","Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.829514\n","Train Epoch: 8 [22400/50000 (45%)]\tLoss: 1.138350\n","Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.926111\n","Train Epoch: 8 [28800/50000 (58%)]\tLoss: 1.218517\n","Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.326963\n","Train Epoch: 8 [35200/50000 (70%)]\tLoss: 0.832871\n","Train Epoch: 8 [38400/50000 (77%)]\tLoss: 1.161056\n","Train Epoch: 8 [41600/50000 (83%)]\tLoss: 0.946910\n","Train Epoch: 8 [44800/50000 (90%)]\tLoss: 1.017477\n","Train Epoch: 8 [48000/50000 (96%)]\tLoss: 0.683444\n","\n","Test set: Average loss: 0.0083, Accuracy: 6445/10000 (64%)\n","\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.186399\n","Train Epoch: 9 [3200/50000 (6%)]\tLoss: 0.706404\n","Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.833079\n","Train Epoch: 9 [9600/50000 (19%)]\tLoss: 0.972096\n","Train Epoch: 9 [12800/50000 (26%)]\tLoss: 1.387427\n","Train Epoch: 9 [16000/50000 (32%)]\tLoss: 1.099565\n","Train Epoch: 9 [19200/50000 (38%)]\tLoss: 1.114154\n","Train Epoch: 9 [22400/50000 (45%)]\tLoss: 0.764441\n","Train Epoch: 9 [25600/50000 (51%)]\tLoss: 1.236679\n","Train Epoch: 9 [28800/50000 (58%)]\tLoss: 1.043283\n","Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.030020\n","Train Epoch: 9 [35200/50000 (70%)]\tLoss: 1.176640\n","Train Epoch: 9 [38400/50000 (77%)]\tLoss: 1.034073\n","Train Epoch: 9 [41600/50000 (83%)]\tLoss: 0.919747\n","Train Epoch: 9 [44800/50000 (90%)]\tLoss: 1.104143\n","Train Epoch: 9 [48000/50000 (96%)]\tLoss: 1.674026\n","\n","Test set: Average loss: 0.0080, Accuracy: 6566/10000 (66%)\n","\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.125571\n","Train Epoch: 10 [3200/50000 (6%)]\tLoss: 0.945288\n","Train Epoch: 10 [6400/50000 (13%)]\tLoss: 1.038147\n","Train Epoch: 10 [9600/50000 (19%)]\tLoss: 0.787684\n","Train Epoch: 10 [12800/50000 (26%)]\tLoss: 1.165028\n","Train Epoch: 10 [16000/50000 (32%)]\tLoss: 0.864405\n","Train Epoch: 10 [19200/50000 (38%)]\tLoss: 1.026203\n","Train Epoch: 10 [22400/50000 (45%)]\tLoss: 1.194893\n","Train Epoch: 10 [25600/50000 (51%)]\tLoss: 1.443561\n","Train Epoch: 10 [28800/50000 (58%)]\tLoss: 0.739781\n","Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.213230\n","Train Epoch: 10 [35200/50000 (70%)]\tLoss: 1.177757\n","Train Epoch: 10 [38400/50000 (77%)]\tLoss: 1.003112\n","Train Epoch: 10 [41600/50000 (83%)]\tLoss: 0.880646\n","Train Epoch: 10 [44800/50000 (90%)]\tLoss: 1.310208\n","Train Epoch: 10 [48000/50000 (96%)]\tLoss: 0.882812\n","\n","Test set: Average loss: 0.0080, Accuracy: 6451/10000 (65%)\n","\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.404863\n","Train Epoch: 11 [3200/50000 (6%)]\tLoss: 0.886868\n","Train Epoch: 11 [6400/50000 (13%)]\tLoss: 0.610144\n","Train Epoch: 11 [9600/50000 (19%)]\tLoss: 0.890331\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1-9ksOb8ykHx","colab":{}},"source":["# if in Google Colab, download your model with this\n","from google.colab import files\n","files.download(\"best.pt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"u1UIFa02y_eH","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}