{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ModelB_0.001.ipynb","provenance":[{"file_id":"103BRRDb5y9_dfYGjX0_e1diwM6Oia4uZ","timestamp":1583682252868},{"file_id":"https://github.com/jblok27/DL_RP5/blob/master/ModelB_Experiment.ipynb","timestamp":1583673409198}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"WUDi6QICWB4v","colab_type":"code","outputId":"5713a68f-d861-4bee-8757-88f954097cd3","executionInfo":{"status":"ok","timestamp":1585151737897,"user_tz":-60,"elapsed":1708,"user":{"displayName":"Jaap Blok","photoUrl":"","userId":"10428959571041432348"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cuda = True\n","train_batch_size = 32\n","test_batch_size = 124\n","best_loss = float(\"inf\")\n","best_epoch = -1\n","dataset_path = './cifar10'\n","gsync_save = True\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-UL2k5sCqFKJ","colab_type":"code","outputId":"d15d120f-b078-48ce-d499-7065151a1bd0","executionInfo":{"status":"ok","timestamp":1585151742292,"user_tz":-60,"elapsed":1528,"user":{"displayName":"Jaap Blok","photoUrl":"","userId":"10428959571041432348"}},"colab":{"base_uri":"https://localhost:8080/","height":219}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torchsummary import summary\n","\n","class ModelB(nn.Module):\n","    def __init__(self, input_size, n_classes=10, **kwargs):\n","        super(ModelB, self).__init__()\n","        self.conv1 = nn.Conv2d(input_size, 96, 5, padding=1)\n","        self.conv2 = nn.Conv2d(96, 96, 1, padding=1)\n","        self.maxPool3=nn.MaxPool2d(3, padding=1, stride=2) \n","        self.conv4 = nn.Conv2d(96, 192, 5, padding=1) \n","        self.conv5 = nn.Conv2d(192, 192, 1, padding=1) \n","        self.maxPool6=nn.MaxPool2d(3,stride=2, padding=1) \n","        self.conv7 = nn.Conv2d(192, 192, 3, padding=1) \n","        self.conv8 = nn.Conv2d(192, 192, 1)\n","        self.class_conv = nn.Conv2d(192, n_classes, 1)\n","    \n","    def forward(self, x):\n","        x_drop = F.dropout(x, .2)\n","        conv1_out = F.relu(self.conv1(x_drop))\n","        conv2_out = F.relu(self.conv2(conv1_out))\n","        maxPool3_out = F.relu(self.maxPool3(conv2_out))\n","        maxPool3_out_drop = F.dropout(maxPool3_out, .5)\n","        conv4_out = F.relu(self.conv4(maxPool3_out_drop))\n","        conv5_out = F.relu(self.conv5(conv4_out))\n","        maxPool6_out = F.relu(self.maxPool6(conv5_out))\n","        maxPool6_out_drop = F.dropout(maxPool6_out, .5)\n","        conv7_out = F.relu(self.conv7(maxPool6_out_drop))\n","        conv8_out = F.relu(self.conv8(conv7_out))\n","\n","        class_out = F.relu(self.class_conv(conv8_out))\n","        pool_out = F.adaptive_avg_pool2d(class_out, 1)\n","        pool_out.squeeze_(-1)\n","        pool_out.squeeze_(-1)\n","        return pool_out\n","\n","trial=ModelB(3)\n","print(trial)\n","\n","#summary(trial.cuda(),(3,32,32))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["AllConvNet(\n","  (conv1): Conv2d(3, 96, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n","  (maxPool3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (conv4): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n","  (conv5): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n","  (maxPool6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv8): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n","  (class_conv): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rZu8tSSYvXVL","colab_type":"code","outputId":"6cbf0aac-2656-436c-9f0e-c61459b03c89","executionInfo":{"status":"ok","timestamp":1585151747838,"user_tz":-60,"elapsed":4665,"user":{"displayName":"Jaap Blok","photoUrl":"","userId":"10428959571041432348"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["cuda =torch.cuda.is_available()\n","trainset = datasets.CIFAR10(root=dataset_path, train=True, download=True)\n","train_mean = trainset.data.mean(axis=(0,1,2))/255  # [0.49139968  0.48215841  0.44653091]\n","train_std = trainset.data.std(axis=(0,1,2))/255  # [0.24703223  0.24348513  0.26158784]\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(train_mean, train_std),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(train_mean, train_std),\n","])\n","kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n","train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(\n","    root=dataset_path, train=True, download=True,\n","    transform=transform_train),\n","    batch_size=train_batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10(root=dataset_path, train=False, download=True,\n","    transform=transform_test),\n","    batch_size=test_batch_size, shuffle=False, **kwargs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3WISJzjRvXVP","colab_type":"code","colab":{}},"source":["model = ModelB(3)\n","if cuda:\n","    model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)\n","scheduler = optim.lr_scheduler.MultiStepLR(\n","    optimizer, milestones=[200, 250, 300], gamma=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5shX88fvvXVR","colab_type":"code","colab":{}},"source":["def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        if cuda:\n","            data, target = data.cuda(), target.cuda()\n","        data, target = Variable(data), Variable(target)\n","\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.data.item()))\n","            \n","def test(epoch, best_loss, best_epoch):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        if cuda:\n","            data, target = data.cuda(), target.cuda()\n","        data, target = Variable(data), Variable(target)\n","\n","        output = model(data)\n","        # sum up batch loss\n","        test_loss += criterion(output, target).data.item()\n","        # get the index of the max log-probability\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print(\n","        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","            test_loss, correct, len(test_loader.dataset), 100. * correct /\n","            len(test_loader.dataset)))\n","    \n","    if test_loss < best_loss:\n","        best_epoch = epoch\n","        best_loss = test_loss\n","        model_save_name = 'ModelB_0_001.pth'\n","        path = F\"/content/gdrive/My Drive/{model_save_name}\"\n","        torch.save(model.state_dict(), path)\n","        print(\"new best\")\n","        print(best_epoch)\n","        #torch.save(model, \"best.pt\")\n","        # if gsync_save:\n","        #     gsync.update_file_to_folder(\"best.pt\")\n","    return best_loss, best_epoch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_YHklUQvXVU","colab_type":"code","colab":{}},"source":["for epoch in range(350):\n","    train(epoch)\n","    scheduler.step()\n","    best_loss, best_epoch = test(epoch, best_loss, best_epoch)\n","\n","#model_save_name = 'ModelB_0_001.pth'\n","#path = F\"/content/gdrive/My Drive/{model_save_name}\"\n","#torch.save(model.state_dict(), path)"],"execution_count":0,"outputs":[]}]}